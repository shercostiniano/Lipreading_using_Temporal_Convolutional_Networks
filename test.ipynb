{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "from collections import deque\n",
    "from contextlib import contextmanager\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import face_alignment\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "\n",
    "from lipreading.model import Lipreading\n",
    "from preprocessing.transform import warp_img, cut_patch\n",
    "\n",
    "STD_SIZE = (256, 256)\n",
    "STABLE_PNTS_IDS = [33, 36, 39, 42, 45]\n",
    "START_IDX = 48\n",
    "STOP_IDX = 68\n",
    "CROP_WIDTH = CROP_HEIGHT = 96\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def VideoCapture(*args, **kwargs):\n",
    "    cap = cv2.VideoCapture(*args, **kwargs)\n",
    "    try:\n",
    "        yield cap\n",
    "    finally:\n",
    "        cap.release()\n",
    "\n",
    "\n",
    "def load_model(config_path: Path):\n",
    "    with open(config_path) as fp:\n",
    "        config = json.load(fp)\n",
    "    tcn_options = {\n",
    "        'num_layers': config['tcn_num_layers'],\n",
    "        'kernel_size': config['tcn_kernel_size'],\n",
    "        'dropout': config['tcn_dropout'],\n",
    "        'dwpw': config['tcn_dwpw'],\n",
    "        'width_mult': config['tcn_width_mult'],\n",
    "    }\n",
    "    return Lipreading(\n",
    "        num_classes=500,\n",
    "        tcn_options=tcn_options,\n",
    "        backbone_type=config['backbone_type'],\n",
    "        relu_type=config['relu_type'],\n",
    "        width_mult=config['width_mult'],\n",
    "        extract_feats=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def visualize_probs(vocab, probs, col_width=4, col_height=300):\n",
    "    num_classes = len(probs)\n",
    "    out = np.zeros((col_height, num_classes * col_width + (num_classes - 1), 3), dtype=np.uint8)\n",
    "    for i, p in enumerate(probs):\n",
    "        x = (col_width + 1) * i\n",
    "        cv2.rectangle(out, (x, 0), (x + col_width - 1, round(p * col_height)), (255, 255, 255), 1)\n",
    "    top = np.argmax(probs)\n",
    "    print(f'Prediction: {vocab[top]}')\n",
    "    print(f'Confidence: {probs[top]:.3f}')\n",
    "    cv2.putText(out, f'Prediction: {vocab[top]}', (10, out.shape[0] - 30), cv2.FONT_HERSHEY_SIMPLEX, fontScale=.5,color=(255, 255, 255))\n",
    "    cv2.putText(out, f'Confidence: {probs[top]:.3f}', (10, out.shape[0] - 10), cv2.FONT_HERSHEY_SIMPLEX, fontScale=.5,color=(255, 255, 255))\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    args = {\n",
    "        'device': 'cpu',\n",
    "        'model_path': 'models/lrw_resnet18_mstcn_video.pth.tar',\n",
    "        'config_path': 'configs/lrw_resnet18_mstcn.json',\n",
    "        'queue_length': 69\n",
    "    }\n",
    "\n",
    "    fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, device=args['device'])\n",
    "    model = load_model(args['config_path'])\n",
    "    model.load_state_dict(torch.load(Path(args['model_path']), map_location=args['device'])['model_state_dict'])\n",
    "    model = model.to(args['device'])\n",
    "\n",
    "    mean_face_landmarks = np.load(Path('preprocessing/20words_mean_face.npy'))\n",
    "\n",
    "    with Path('labels/500WordsSortedList.txt').open() as fp:\n",
    "        vocab = fp.readlines()\n",
    "    assert len(vocab) == 500\n",
    "\n",
    "    queue = deque(maxlen=args['queue_length'])\n",
    "\n",
    "    with VideoCapture(r'/C:\\Users\\sherw\\OneDrive\\Desktop\\Thesis\\filipino-lipreading\\sample.mp4') as cap:\n",
    "        Patch_imshow_index = 1\n",
    "        Vis_imshow_index = 1\n",
    "        Camera_imshow_index = 1\n",
    "        \n",
    "        length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        print(length)\n",
    "        while True:\n",
    "            ret, image_np = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            image_np = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            all_landmarks = fa.get_landmarks(image_np)\n",
    "            if all_landmarks:\n",
    "                landmarks = all_landmarks[0]\n",
    "\n",
    "                # BEGIN PROCESSING\n",
    "\n",
    "                trans_frame, trans = warp_img(\n",
    "                    landmarks[STABLE_PNTS_IDS, :], mean_face_landmarks[STABLE_PNTS_IDS, :], image_np, STD_SIZE)\n",
    "                trans_landmarks = trans(landmarks)\n",
    "                patch = cut_patch(\n",
    "                    trans_frame, trans_landmarks[START_IDX:STOP_IDX], CROP_HEIGHT // 2, CROP_WIDTH // 2)\n",
    "\n",
    "                cv2.imshow('patch', cv2.cvtColor(patch, cv2.COLOR_RGB2BGR))\n",
    "                path=\"testing_picture_outputs/\" + \"mouth_\"+ str(Patch_imshow_index) + '.jpg'\n",
    "                cv2.imwrite(path, cv2.cvtColor(patch, cv2.COLOR_RGB2BGR))\n",
    "                Patch_imshow_index += 1\n",
    "\n",
    "                patch_torch = to_tensor(cv2.cvtColor(patch, cv2.COLOR_RGB2GRAY)).to(args['device'])\n",
    "                queue.append(patch_torch)\n",
    "\n",
    "                # if len(queue) >= args['queue_length']:\n",
    "                #     with torch.no_grad():\n",
    "                #         model_input = torch.stack(list(queue), dim=1).unsqueeze(0)\n",
    "                #         logits = model(model_input, lengths=[args['queue_length']])\n",
    "                #         probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "                #         probs = probs[0].detach().cpu().numpy()\n",
    "\n",
    "                #     vis = visualize_probs(vocab, probs)\n",
    "                    # cv2.imshow('probs', vis)\n",
    "                    # path = \"testing_picture_outputs/\" + \"Vis_Test_Out_\"+ str(Vis_imshow_index) + '.jpg'\n",
    "                    # cv2.imwrite(path, vis)\n",
    "                    # Vis_imshow_index += 1\n",
    "\n",
    "                # END PROCESSING\n",
    "\n",
    "                # for x, y in landmarks:\n",
    "                    # cv2.circle(image_np, (int(x), int(y)), 2, (0, 0, 255))\n",
    "\n",
    "            # cv2.imshow('camera', cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR))\n",
    "            # path = \"testing_picture_outputs/\" + \"Camera_Test_Out_\"+ str(Camera_imshow_index) + '.jpg'\n",
    "            # cv2.imwrite(path, cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR))\n",
    "            # Camera_imshow_index += 1\n",
    "\n",
    "            key = cv2.waitKey(1)\n",
    "            if key in {27, ord('q')}:  # 27 is Esc\n",
    "                break\n",
    "            elif key == ord(' '):\n",
    "                cv2.waitKey(0)\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a0c5b2d83942f5d81cb466efd8ac78da06ad7a2514af390ec957933e585d8b9f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
